---
layout: post  
title: 基于知识图谱的语义相似度分析  
date: 2018-09-22
catalog: true
author: Yun
mathjax: true
tags:
    - 知识图谱
    - 知识图谱应用
    - 语义分析
---





## 借助知识图谱的语义相似度分析

语义相似性是我们解决很多问题的基石。比如说网页和文本检索，问答系统，文本分析和理解。在此之前，我们需要区分语义相关性和语义相似性。

如果两个单词在意义上是相近的，他们所属的概念共享很多相同的属性，那我们称之为语义相似性。例如微软和谷歌在语义上就是相似的，因为他们都是公司，更加精细的，都是科技公司。

如果两个单词是相关的，我们就称之为语义相关性。比如说汽车和旅行便是语义相关的，因为汽车是旅行的一种交通方式。

语义相似度分析主要有两种方式：基于知识的分析和基于语料库的分析。基于知识的分析往往是通过一些人工构建的知识库中中的isA关系进行语义距离分析。基于语料库的分析往往是通过n-grams等语言模型进行分析。

正如深度学习对数据的需求一样，知识库或者语料库的质量很大程度上决定了结果的好坏。然而目前的知识库，例如手工构建的WordNet，依然存在着覆盖范围低，更新缓慢的问题。

基于语料库的分析，也依然存在着一些问题。

- 语料往往存在着一些偏见，通过搜索引擎得到的数据往往聚焦于较为频繁的词义。
- 语言模型通常是通过左右两边的词汇来推断可能出现的单词，这更倾向于相关性而非相似性。
- 语言模型的粒度往往是单词级别的，然而很多实例是词汇，割裂成单词后失去了原本的含义
- 歧义问题难以解决
- 很多模型是基于收索引擎的结果进行分析的，速度难以接受

微软通过自己的概念知识图谱Probase提出了基于概念网络（isA）的语义相似性分析方法。



#### 语义网络

Probase是微软构建的概念知识图谱。Probase包含了实体和概念之间的isA关系，以及概念与概念之间的上下位关系。


$$
\Gamma_{isA} = \{<c,e>\}
$$
C是上位词而E是下义词。例如微软是一家公司。“公司”是一个概念，而“微软”是这个概念下的一个实体。我们可以用（微软，isA, 公司）这个三元组来表示这个关系。





#### 概念和实体表示

每一对isA关系（e isA c）都关联着一个代表其典型性的条件概率。
$$
P(e|c) = \frac{\mbox{occurences of (c,e) in Hearst extraction}}{\mbox{occurences of c in Hearst extraction}}
$$
对于两个单词<t_1,t_2>，我们首先判断他们的类型（概念，实体）。接着获取他们的上下文。
$$
SIM(t_1,t_2)=sim(T(t_1),T(t_2))
$$

在计算两个词汇的语义相似性之前，我们需要先解决那些形不同而神同的词汇。

**同义词**：&lt; GE,General Electric&gt; ,&lt; company,corporation,firm&gt; 

**格式问题**:&lt; 2d bar code,2d bar code&gt; ,&lt; accomplished artist,accomplished artiste&gt; 

**单复数形式**：&lt;shoe,shoes &gt; 

首先，通过wikipedia和WordNet的重定向，内联链接，同义词集合来组合我们的同义词

接着，通过编辑距离来判断t1和t2的表达形式上距离。
$$
dist_{lex}(t1,t2) =\frac{\mbox{EditDistance(t1,t2)}}{\mbox{MaxLength(t1,t2)}}
$$
如果这个距离小于某个阈值（此处为0.05），那我们可以判断他们在表达形式上是极其相似的，从而将他们组合在一起。

这样，我们就得到了一个和WordNet的同义词集类似的集合，我们称之为$\Gamma_{ssyn}$。当两个词汇属于同一同义词集中时，我们认为它们是完全相同的，即他们的相似度为1。

我们根据词汇的类型以及它在语义网络中的位置来获取它的上下文。如果是一个概念，它所涵括的所有的实体就是它的上下文。如果是一个实体，那么所有它属于的概念就是它的上下文。
$$
I_c = <w_1^{'},\cdots,w_k^{'}>
$$

$$
I_e = <w_1,\cdots,w_k>
$$

$$
sim(T(t_1),T(t_2))=cosine(T(t_1),T(t_2))
$$

```
如果t1,t2属于相同的同义词集，那么返回相似度为1
类型判断；
如果<t1,t2>是概念对：
	获取t1和t2的所有实体，并计算t1和t2所代表的向量，计算相似度
如果<t1,t2>是实体对：
	获取t1和t2的所有概念，并计算t1和t2所代表的向量，计算相似度
如果<t1,t2>是概念-实体对：
	从isA关系对中获取ti的前K个最有代表性的概念，计算相似度并返回最大的那个
```



#### 存在的问题

我们可以发现，在面对带有歧义概念的实体时，算法的偏向性非常明显。如果我们的语料库是个科技达人，那么“苹果”这个实体往往更倾向于“科技公司”这一概念。如果我们的语料库更吃货一点，那么更倾向于“水果”这一概念。产生这一结果的原因是我们直接对实体的所有的概念根据其典型性进行相似性分析，导致在语料库中更加典型的概念在相似度计算时权重更大。同样，歧义的概念也会稀疏原本语义相似的实体的相似性。

#### 概念聚类

![cluster](https://github.com/FeiLiYun/feiliyun.github.io/raw/master/img/cluster.png)

如(b)所示，为了消除语义上的歧义，我们需要将"苹果"这个实体所对应的概念，分为四个语义不相同的组以消除不同语义之间的影响。

这下我们的问题变为如何聚类相似的概念，这里我们通过k-Medoids聚类算法对该实体对应的所有概念进行划分。

首先我们定义两个概念（c1,c2）的语义距离为
$$
d_{sem}(c_1,c_2)=1-cosine(I_{c_1},I_{c_2})
$$



由于k-Medoids是一个时间复杂度为O(kn^2)的算法。其中k为质心数，n为点（概念）的数量。为了提高查询的效率，我们可以事先对Probase里的所有概念进行聚类。这样我们可以将查询的时间复杂度变为O(n)。

对整个知识库的操作意味着我们需要建立一边是所有概念，一边是所有实例的一个二分图。对每一个概念，我们将其表示为一个经过L2标准化的，每一维代表一个实例。

由于每一个概念所对应的实体并不是很多，因此这个二分图实际是非常稀疏的，同时我们可以将那些代表性非常低的边当作噪音去掉以加快计算。



#### 优化

1. 一些概念属于噪音，会对最后的结果产生影响。
2. 过高层次的抽象概念会导致使得原本语义不相似的一组词汇变得相似。例如“午饭”和“音乐”这两个概念共享一个上位概念“活动”。

对于第二个问题，如果存在一个概念是集合中其它概念的上位概念，我们将它移除即可。这样我们剩下的便是更为精细的概念。

