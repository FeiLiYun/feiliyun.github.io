---
layout: post 
title: 基于知识图谱的语义相似度分析 
date: 2018-09-22
catalog: true
author: Yun
mathjax: true
tags:
    - 知识图谱
    - 知识图谱基础
    - 语义分析
---




## 基于知识图谱的语义相似性分析

### 前言

语义相似性分析是解决很多问题的基石。比如说网页和文本检索，问答系统，文本分析和理解。在此之前，我们需要区分语义相关性和语义相似性。

如果两个单词在意义上是相近的，他们所属的概念共享很多相同的属性，那我们称之为语义相似性。例如微软和谷歌在语义上就是相似的，因为他们都是公司，更加精细的，都是科技公司。

如果两个单词是相关的，我们就称之为语义相关性。比如说汽车和旅行便是语义相关的，因为汽车是旅行的一种交通方式。

语义相似度分析主要有两种方式：基于知识的分析和基于语料库的分析。基于知识的分析往往是通过一些人工构建的知识库中中的isA关系进行语义距离分析。基于语料库的分析往往是通过n-grams等语言模型进行分析。

正如深度学习对数据的需求一样，知识库或者语料库的质量很大程度上决定了结果的好坏。然而目前的知识库，例如手工构建的WordNet，依然存在着覆盖范围低，更新缓慢的问题。

基于语料库的分析，也依然存在着一些问题。

- 语料往往存在着一些偏见，通过搜索引擎得到的数据往往聚焦于较为频繁的词义。
- 语言模型通常是通过左右两边的词汇来推断可能出现的单词，这更倾向于相关性而非相似性。
- 语言模型的粒度往往是单词级别的，然而很多实例是词汇，割裂成单词后失去了原本的含义
- 歧义问题难以解决
- 很多模型是基于收索引擎的结果进行分析的，速度难以接受

微软通过自己的概念知识图谱Probase提出了基于概念网络（isA）的语义相似性分析方法。



### 语义网络

Probase是微软构建的概念知识图谱。Probase包含了实体和概念之间的isA关系，以及概念与概念之间的上下位关系。

$$
\Gamma_{isA} = \{<c,e>\}
$$

$c$是上位词，$e$是下义词。例如微软是一家公司。“公司”是一个概念，而“微软”是这个概念下的一个实体。我们可以用（微软，isA, 公司）这个三元组来表示这个关系。






### 基于实体（概念）典型性的相似性分析算法



#### 同义词集合

我们先构建同义词集$\Gamma_{ssyn}$，同义词集中的实例（概念）我们默认它们的相似度为1。因为它们仅仅是形式的不同，在语义上则是完全相同的。例如：

- **同义词**：&lt; GE,General Electric&gt; ,&lt; company,corporation,firm&gt; 

- **格式问题**:&lt; 2d bar code,2d bar code&gt; ,&lt; accomplished artist,accomplished artiste&gt; 

- **单复数形式**：&lt;shoe,shoes &gt; 

找寻同义词的过程我们可以利用第三方知识库和编辑距离公式。

1. 通过wikipedia和WordNet的重定向，内联链接，同义词集合来组合我们的同义词
2. 通过编辑距离来判断t1和t2的表达形式上距离。

$$
dist_{lex}(t1,t2) =\frac{\mbox{EditDistance(t1,t2)}}{\mbox{MaxLength(t1,t2)}}
$$

3. 如果这个距离小于某个阈值（0.05），那我们可以认定他们是同义词。



#### 上下文表示

每一对实例-概念关系（e isA c）都关联着一个代表其典型性的条件概率。

$$
P(e|c) = \frac{\mbox{语料库中 (c,e) 出现次数}}{\mbox{语料库中概念c出现次数}}
$$

对于词汇对$t_1,t_2$，我们先判断他们是概念或实体。接着获取他们的上下文表示用于相似性计算。

如果是概念，它的上下文$I_c$可以用它所涵括的实例表示。

$$
I_c = <w_1^{'},\cdots,w_k^{'}>
$$

其中$w_i^{'} =p(e_i\mid  c)$，$p(e_i\mid c)$表示$e_i$在$c$所涵括的所有实例里的典型性（出现概率）。

同理，对于实例，它的上下文$I_e$可以用它所属的概念表示。

$$
I_e = <w_1,\cdots,w_k>
$$

其中$w_i =p(c_i \mid e)$，$p(c_i\mid e)$表示$c_i$在$e$所代表的所有概念里的典型性（出现概率）。

我们用**余弦相似度**来计算两个向量的相似性。

$$
sim(T(t_1),T(t_2))=cosine(I_1,I_2)
$$



我们以苹果和微软这对实例为例子（从微软probase中获取，去掉了概率低的概念）。


$$
\begin{array}{c|lll}
{}&{水果}&{公司}&{食物}&{供应商}\\
\hline
{苹果}&{0.41}&{0.28}&{0.07}&{0}\\
{微软}&{0}&{0.63}&{0}&{0.085}\\
\end{array}
\quad cos(I_{苹果}，I_{微软}) = 0.5534241227343059
\\
\\
\begin{array}{c|lll}
{}&{水果}&{公司}&{食物}&{供应商}\\
\hline
{苹果}&{0.28}&{0.41}&{0.07}&{0}\\
{微软}&{0}&{0.63}&{0}&{0.085}\\
\end{array}
\quad cos(I_{苹果}，I_{微软}) = 0.8103710368609477
$$



我们可以发现，在面对带有歧义概念的实体时，算法的偏向性非常明显。如果我们的语料库科技类新闻较多，那么“苹果”这个实例往往更倾向于“公司”这一概念。如果我们的语料库生活类新闻较多，那么更倾向于“水果”这一概念。实际上，苹果这一实体是具有歧义性的，所以即可以和微软相似，也可以和梨子相似。将其所有的概念简单归纳在一个向量里进行计算是不妥的。



### 概念聚类和最大化相似度算法

#### 概念聚类

![cluster](https://github.com/FeiLiYun/feiliyun.github.io/raw/master/img/cluster.png)

如(b)所示，为了消除语义上的歧义，我们需要将"苹果"这个实体所对应的概念，分为四个语义不相同的组以消除不同语义之间的影响。

我们通过k-Medoids聚类算法对该实体对应的所有概念进行划分。

首先我们定义两个概念$（c1,c2）$的语义距离为

$$
d_{sem}(c_1,c_2)=1-cosine(I_{c_1},I_{c_2})
$$



由于k-Medoids是一个时间复杂度为$O(kn^2)$算法。其中$k$为质心数，$n$为点（概念）的数量。为了提高查询的效率，我们可以事先对Probase里的所有概念进行聚类。这样我们可以将查询的时间复杂度变为$O(n)$ 。

对整个知识库的操作意味着我们需要建立一边是所有概念，一边是所有实例的一个二分图。对每一个概念，我们将其表示为一个经过L2标准化的，每一维代表一个实例。

由于每一个概念所对应的实体并不是很多，因此这个二分图实际是非常稀疏的，同时我们可以将那些代表性非常低的边当作噪音去掉以加快计算。

#### 最大相似度算法

对整个知识库进行聚类操作后，我们得到了一个集合K = {K_1,K_2,\cdots,K_k}。根据K，我们可以得到t_1和t_2的聚类结果。

$$
sup(t_i) = \{c\mid \langle c,t_i \rangle \in F_{isA}\}\\
K_{t_1} = \{x\mid x=K_i\cap sup(t_1),\forall K_i\in K \wedge x \not = \emptyset \}\\
K_{t_2} = \{x\mid x=K_i\cap sup(t_2),\forall K_i\in K \wedge x \not = \emptyset \}\\
sim(T(t_1),T(t_2)) = \max_{x\in K_{t_1},y \in k_{t_2}}\{cosine(x,y)\}
$$

简而言之，对于两个实例各自聚类后的集合进行一一相似度计算，返回最大的值即为两个实例的相似度。



#### 算法优化

在最大相似度算法里，依然存在一些问题

1. 一些概念属于噪音，会对最后的结果产生影响。
2. 代表性过高的抽象概念会导致使得原本语义不相似的一组词汇变得相似。例如“午饭”和“音乐”这两个概念共享一个上位概念“活动”。如果活动在这两个实例里的代表性很高，那么这两个实例会被认定为相似。

我们可以从以下几个方面解决

1. 合并属于同义词集$F_{ssyn}$的概念（实例），其典型性相加。
2. 对于代表性小于某一阈值的概念（实例），视为噪音，从概念（实例）集中删除。
3. 遍历实例的概念集，如果某个概念是另一个概念的上位概念，那么从概念集中删除上位概念。



### 基于实体（概念）典型性的可消岐相似性分析算法



### 原文

Li P, Wang H, Zhu K Q, et al. Computing term similarity by large probabilistic isA knowledge[J]. 2013:1401-1410.



